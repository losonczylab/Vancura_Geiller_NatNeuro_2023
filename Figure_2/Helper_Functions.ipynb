{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simpleaxis(ax):\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.get_xaxis().tick_bottom()\n",
    "    ax.get_yaxis().tick_left()\n",
    "    for axis in ['top','bottom','left','right']:\n",
    "        ax.spines[axis].set_linewidth(0.5)\n",
    "    ax.tick_params(width=0.5)\n",
    "\n",
    "def ROI_planeID(expt, label):\n",
    "    signals = expt.imaging_dataset().signals()[label]['rois']\n",
    "    nROIs = len(signals)\n",
    "    planeID = []\n",
    "    for i in xrange(nROIs):\n",
    "        planeID.append(signals[i]['polygons'][0][0][2])\n",
    "\n",
    "    return np.asarray(planeID).astype('int')\n",
    "\n",
    "def resample_trace(fluorescence_trace, number_of_data_points):\n",
    "    y = fluorescence_trace\n",
    "    x = np.arange(0, len(y))\n",
    "    f = interpolate.interp1d(x,y)\n",
    "    xnew = np.linspace(x.min(), x.max(), num = number_of_data_points)\n",
    "    ynew = f(xnew)\n",
    "    return ynew\n",
    "\n",
    "def calculate_Cross_Correlation(expt, cell, velocity, number_of_lag_seconds = 5):\n",
    "    mean_centered_velocity = velocity - np.nanmean(velocity)\n",
    "    mean_centered_cell_trace = cell - np.nanmean(cell)\n",
    "    lag_start = -(int(np.rint(1/expt.frame_period() * number_of_lag_seconds)))\n",
    "    lag_stop = int(np.rint(1/expt.frame_period() * number_of_lag_seconds)) + 1\n",
    "    correlation_coefficients = []\n",
    "    shifts = []\n",
    "    for i in range(lag_start, lag_stop):\n",
    "        shifted_trace = shift(mean_centered_cell_trace, i, mode = 'constant', order = 0, cval = 0.0)\n",
    "        correlation_coefficient = ma.corrcoef(ma.masked_invalid(shifted_trace), \\\n",
    "                                            ma.masked_invalid(mean_centered_velocity))[0][1]\n",
    "        #correlation_coefficient = np.corrcoef(mean_centered_velocity, y = shifted_trace)[0][1]\n",
    "        shifts.append(i)\n",
    "        correlation_coefficients.append(correlation_coefficient)\n",
    "    shifts = np.asarray(shifts) / float(1/expt.frame_period())\n",
    "    ordered_corr_coefs = np.reshape(np.array(list(it.izip(shifts, correlation_coefficients))), (-1,2))\n",
    "    corr_coef_dataframe = pd.DataFrame(data = ordered_corr_coefs, index = None, columns = ['Shift', 'Corr_Coef'])\n",
    "    max_abs_value_corr_coef_index = corr_coef_dataframe.Corr_Coef.abs().idxmax()\n",
    "    if ~np.isnan(max_abs_value_corr_coef_index):\n",
    "        lag = corr_coef_dataframe.iloc[max_abs_value_corr_coef_index]['Shift']\n",
    "        corr_coef = corr_coef_dataframe.iloc[max_abs_value_corr_coef_index]['Corr_Coef']\n",
    "        return corr_coef, lag \n",
    "    else:\n",
    "        return np.nan, np.nan\n",
    "    \n",
    "def find_putative_run_intervals(velocity_array, velocity_cut_off = 0.2): \n",
    "    putative_run_start_frames = []\n",
    "    for i in range(len(velocity_array)-1):\n",
    "        if velocity_array[i] <= velocity_cut_off and velocity_array[i+1] >= velocity_cut_off: \n",
    "            putative_run_start_frames.append(i+1)\n",
    "    putative_run_stop_frames = []\n",
    "    for i in range(len(velocity_array)-1):\n",
    "        if velocity_array[i] >= velocity_cut_off and velocity_array[i+1] < velocity_cut_off:\n",
    "            putative_run_stop_frames.append(i+1)\n",
    "    if putative_run_stop_frames[0] < putative_run_start_frames[0]:\n",
    "        #print(\"Appending 0 to start\")\n",
    "        putative_run_start_frames.insert(0,0)\n",
    "    if putative_run_stop_frames[-1] < putative_run_start_frames[-1]:\n",
    "        #print(\"Appending last frame to end\")\n",
    "        putative_run_stop_frames.append(len(velocity_array) - 1)\n",
    "    putative_run_intervals = np.array(list(it.izip(putative_run_start_frames,putative_run_stop_frames)))\n",
    "    return putative_run_intervals \n",
    "\n",
    "def merge_nearby_intervals(run_intervals, minimum_interval_separation = 20):\n",
    "    flattened_intervals = run_intervals.flatten()\n",
    "    indices_to_delete = []\n",
    "    for i, difference in enumerate(np.diff(flattened_intervals)):\n",
    "        if i % 2 != 0 and difference < minimum_interval_separation: ###\n",
    "            indices_to_delete.append(i)\n",
    "            indices_to_delete.append(i + 1)\n",
    "    merged_run_intervals = np.delete(run_intervals, indices_to_delete, axis = None)\n",
    "    merged_run_intervals = np.reshape(merged_run_intervals, (-1,2))\n",
    "    return merged_run_intervals \n",
    "\n",
    "def remove_short_intervals(run_intervals, minimum_interval_length = 5):\n",
    "    short_intervals = [] \n",
    "    for i in range(len(run_intervals)):\n",
    "        if run_intervals[i,1] - run_intervals[i,0] < minimum_interval_length:\n",
    "            short_intervals.append(i)\n",
    "    filtered_run_intervals = np.delete(run_intervals, short_intervals, axis = 0)\n",
    "    return filtered_run_intervals\n",
    "\n",
    "def remove_small_amplitude_intervals(velocity_array, run_intervals, velocity_threshold = 0.5):\n",
    "    small_amplitude_intervals = []\n",
    "    for i in range(len(run_intervals)):\n",
    "        if max(velocity_array[run_intervals[i,0]:run_intervals[i,1]]) < velocity_threshold:\n",
    "            small_amplitude_intervals.append(i)\n",
    "    filtered_run_intervals = np.delete(run_intervals, small_amplitude_intervals, axis = 0)\n",
    "    return filtered_run_intervals \n",
    "\n",
    "def get_average_run_start_trace(expt, cell, velocity, velocity_cut_off = 0.2, \\\n",
    "    length_of_window_in_seconds = 3, number_of_data_points = 50):\n",
    "    putative_intervals = find_putative_run_intervals(velocity, velocity_cut_off = velocity_cut_off)\n",
    "    int1 = merge_nearby_intervals(putative_intervals)\n",
    "    int2 = remove_short_intervals(int1)\n",
    "    final_intervals = remove_small_amplitude_intervals(velocity,int2)\n",
    "    run_start_frames = []\n",
    "    for i, interval in enumerate(final_intervals):\n",
    "        run_start_frames.append(interval[0])\n",
    "    run_start_frames = [int(i) for i in run_start_frames]\n",
    "    \n",
    "    run_start_traces = []\n",
    "    window_length = int(np.rint(1/expt.frame_period() * length_of_window_in_seconds)) + 1\n",
    "    for run_start in run_start_frames:\n",
    "        if ((run_start + window_length) < len(cell)) & (run_start - window_length > 0): \n",
    "            run_start_traces.append(cell[run_start - window_length : run_start + window_length])\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    resampled_run_start_traces = []\n",
    "    for run_start_trace in run_start_traces:\n",
    "        resampled_run_start_traces.append(resample_trace(run_start_trace, number_of_data_points))\n",
    "    resampled_run_start_traces = np.asarray(resampled_run_start_traces)\n",
    "\n",
    "    average_run_start_trace = np.nanmean(resampled_run_start_traces, axis = 0)\n",
    "    return average_run_start_trace\n",
    "\n",
    "def get_average_run_stop_trace(expt, cell, velocity, velocity_cut_off = 0.2, \\\n",
    "    length_of_window_in_seconds = 3, number_of_data_points = 50):\n",
    "    putative_intervals = find_putative_run_intervals(velocity, velocity_cut_off = velocity_cut_off)\n",
    "    int1 = merge_nearby_intervals(putative_intervals)\n",
    "    int2 = remove_short_intervals(int1)\n",
    "    final_intervals = remove_small_amplitude_intervals(velocity,int2)\n",
    "    run_stop_frames = []\n",
    "    for i, interval in enumerate(final_intervals):\n",
    "        run_stop_frames.append(interval[1])\n",
    "    run_stop_frames = [int(i) for i in run_stop_frames]\n",
    "\n",
    "    run_stop_traces = []\n",
    "    window_length = int(np.rint(1/expt.frame_period() * length_of_window_in_seconds)) + 1\n",
    "    for run_stop in run_stop_frames:\n",
    "        if ((run_stop + window_length) < len(cell)) & (run_stop - window_length > 0): \n",
    "            run_stop_traces.append(cell[run_stop - window_length : run_stop + window_length])\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    resampled_run_stop_traces = []\n",
    "    for run_stop_trace in run_stop_traces:\n",
    "        resampled_run_stop_traces.append(resample_trace(run_stop_trace, number_of_data_points))\n",
    "    resampled_run_stop_traces = np.asarray(resampled_run_stop_traces)\n",
    "\n",
    "    average_run_stop_trace = np.nanmean(resampled_run_stop_traces, axis = 0)\n",
    "    return average_run_stop_trace  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
